{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0587398d",
   "metadata": {},
   "source": [
    "sudo apt update\n",
    "sudo apt install -y python3-venv python3-pip\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install pi-ina219 torch pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da00818",
   "metadata": {},
   "source": [
    "## VERS√ÉO PURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab598e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (sem otimiza√ß√µes) ===\n",
      "Epoch 01/100  - Train MSE: 6.248398\n",
      "Epoch 05/100  - Train MSE: 0.270467\n",
      "Epoch 10/100  - Train MSE: 0.262171\n",
      "Epoch 15/100  - Train MSE: 0.258560\n",
      "Epoch 20/100  - Train MSE: 0.264250\n",
      "Epoch 25/100  - Train MSE: 0.264546\n",
      "Epoch 30/100  - Train MSE: 0.257848\n",
      "Epoch 35/100  - Train MSE: 0.268403\n",
      "Epoch 40/100  - Train MSE: 0.255421\n",
      "Epoch 45/100  - Train MSE: 0.264791\n",
      "Epoch 50/100  - Train MSE: 0.259054\n",
      "Epoch 55/100  - Train MSE: 0.253974\n",
      "Epoch 60/100  - Train MSE: 0.256649\n",
      "Epoch 65/100  - Train MSE: 0.261080\n",
      "Epoch 70/100  - Train MSE: 0.255296\n",
      "Epoch 75/100  - Train MSE: 0.252065\n",
      "Epoch 80/100  - Train MSE: 0.255939\n",
      "Epoch 85/100  - Train MSE: 0.253428\n",
      "Epoch 90/100  - Train MSE: 0.254143\n",
      "Epoch 95/100  - Train MSE: 0.256427\n",
      "Epoch 100/100  - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline (sem otimiza√ß√µes):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.156658 s\n",
      "Tempo m√©dio por amostra:   0.0261 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_baseline_ina_50runs.py\n",
    "# ============================================================\n",
    "# CNN 1D (PyTorch) + MEDI√á√ÉO INA219 (infer√™ncia) ‚Äî 50 rodadas\n",
    "# Salva m√©tricas e energia de cada rodada em CSV.\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- INA219 ---\n",
    "try:\n",
    "    from ina219 import INA219, DeviceRangeError\n",
    "    INA_AVAILABLE = True\n",
    "except Exception:\n",
    "    INA_AVAILABLE = False\n",
    "    print(\"[AVISO] pi-ina219 n√£o encontrado. Rode: pip install pi-ina219\")\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE = 100\n",
    "LR_BASELINE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # for√ßado CPU\n",
    "\n",
    "# INA219\n",
    "SHUNT_OHMS = 0.1      # ohms (ajuste conforme seu breakout)\n",
    "I2C_ADDRESS = 0x40     # endere√ßo padr√£o do INA219\n",
    "SAMPLE_INTERVAL = 0.01 # 10 ms\n",
    "\n",
    "# Bench\n",
    "N_RUNS = 50\n",
    "OUT_CSV = \"resultados_inferencia_ina219.csv\"\n",
    "SLEEP_BETWEEN_RUNS = 0.20  # segundos, pequeno intervalo p/ estabilizar consumo\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 16*length)\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)    # (N,)\n",
    "\n",
    "# ----------------------- Medidor de Energia (INA219) -----------------------\n",
    "class EnergyMeter:\n",
    "    \"\"\"\n",
    "    Amostra V (volts), I (mA), P (W) em intervalos fixos.\n",
    "    Calcula energia (J) integrando P*dt ao longo do per√≠odo de medi√ß√£o.\n",
    "    \"\"\"\n",
    "    def __init__(self, shunt_ohms=0.1, address=0x40, sample_interval=0.01):\n",
    "        self.enabled = INA_AVAILABLE\n",
    "        self.sample_interval = sample_interval\n",
    "        self._thread = None\n",
    "        self._stop = threading.Event()\n",
    "        self.samples = deque()  # (timestamp, volts, mA, watts)\n",
    "        self.start_t = None\n",
    "        self.end_t = None\n",
    "\n",
    "        if self.enabled:\n",
    "            try:\n",
    "                self.ina = INA219(shunt_ohms, address=address)\n",
    "                self.ina.configure()  # 32V, ganho auto, 12-bit\n",
    "            except Exception as e:\n",
    "                print(f\"[AVISO] Falha ao inicializar INA219: {e}\")\n",
    "                self.enabled = False\n",
    "\n",
    "    def _sample_loop(self):\n",
    "        self.start_t = time.perf_counter()\n",
    "        while not self._stop.is_set():\n",
    "            t = time.perf_counter()\n",
    "            try:\n",
    "                volts = self.ina.voltage()              # V\n",
    "                current_mA = self.ina.current()         # mA\n",
    "                watts = (current_mA / 1000.0) * volts   # W\n",
    "                self.samples.append((t, volts, current_mA, watts))\n",
    "            except DeviceRangeError:\n",
    "                self.samples.append((t, float(\"nan\"), float(\"nan\"), float(\"nan\")))\n",
    "            except Exception:\n",
    "                self.samples.append((t, float(\"nan\"), float(\"nan\"), float(\"nan\")))\n",
    "            time.sleep(self.sample_interval)\n",
    "        self.end_t = time.perf_counter()\n",
    "\n",
    "    def start(self):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        self.samples.clear()\n",
    "        self._stop.clear()\n",
    "        self._thread = threading.Thread(target=self._sample_loop, daemon=True)\n",
    "        self._thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        self._stop.set()\n",
    "        if self._thread is not None:\n",
    "            self._thread.join(timeout=2.0)\n",
    "\n",
    "    def summarize(self):\n",
    "        if not self.enabled or len(self.samples) < 2:\n",
    "            return {\n",
    "                \"duration_s\": None,\n",
    "                \"energy_J\": None,\n",
    "                \"avg_power_W\": None,\n",
    "                \"peak_power_W\": None,\n",
    "                \"avg_current_mA\": None,\n",
    "                \"peak_current_mA\": None,\n",
    "                \"avg_voltage_V\": None,\n",
    "                \"n_samples\": len(self.samples)\n",
    "            }\n",
    "\n",
    "        arr = np.array(self.samples, dtype=float)\n",
    "        t = arr[:, 0]\n",
    "        V = arr[:, 1]\n",
    "        I_mA = arr[:, 2]\n",
    "        P_W = arr[:, 3]\n",
    "\n",
    "        mask = np.isfinite(P_W)\n",
    "        t_valid = t[mask]\n",
    "        P_valid = P_W[mask]\n",
    "\n",
    "        energy_J = 0.0\n",
    "        if len(P_valid) >= 2:\n",
    "            dt = np.diff(t_valid)\n",
    "            P_mid = (P_valid[:-1] + P_valid[1:]) / 2.0\n",
    "            energy_J = float(np.sum(P_mid * dt))\n",
    "\n",
    "        duration_s = (self.end_t - self.start_t) if (self.end_t and self.start_t) else float(t[-1] - t[0])\n",
    "        avg_power_W = float(np.nanmean(P_W)) if np.any(np.isfinite(P_W)) else None\n",
    "        peak_power_W = float(np.nanmax(P_W)) if np.any(np.isfinite(P_W)) else None\n",
    "        avg_current_mA = float(np.nanmean(I_mA)) if np.any(np.isfinite(I_mA)) else None\n",
    "        peak_current_mA = float(np.nanmax(I_mA)) if np.any(np.isfinite(I_mA)) else None\n",
    "        avg_voltage_V = float(np.nanmean(V)) if np.any(np.isfinite(V)) else None\n",
    "\n",
    "        return {\n",
    "            \"duration_s\": float(duration_s),\n",
    "            \"energy_J\": energy_J,\n",
    "            \"avg_power_W\": avg_power_W,\n",
    "            \"peak_power_W\": peak_power_W,\n",
    "            \"avg_current_mA\": avg_current_mA,\n",
    "            \"peak_current_mA\": peak_current_mA,\n",
    "            \"avg_voltage_V\": avg_voltage_V,\n",
    "            \"n_samples\": int(len(self.samples))\n",
    "        }\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs}  - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model_with_energy(model, loader, label=\"Avalia√ß√£o (com energia)\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    meter = EnergyMeter(shunt_ohms=SHUNT_OHMS, address=I2C_ADDRESS, sample_interval=SAMPLE_INTERVAL)\n",
    "\n",
    "    # Pr√©-aquecimento r√°pido (evita primeira chamada fria afetar muito)\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            _ = model(xb)\n",
    "            break\n",
    "\n",
    "    if meter.enabled:\n",
    "        meter.start()\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "    if meter.enabled:\n",
    "        meter.stop()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    energy_stats = meter.summarize() if meter.enabled else None\n",
    "\n",
    "    result = {\n",
    "        \"mae\": _mae,\n",
    "        \"rmse\": _rmse,\n",
    "        \"r2\": _r2,\n",
    "        \"total_inference_s\": total_time,\n",
    "        \"time_per_sample_ms\": time_per_sample_ms,\n",
    "    }\n",
    "    if energy_stats:\n",
    "        result.update({\n",
    "            \"duration_s\": energy_stats[\"duration_s\"],\n",
    "            \"energy_J\": energy_stats[\"energy_J\"],\n",
    "            \"avg_power_W\": energy_stats[\"avg_power_W\"],\n",
    "            \"peak_power_W\": energy_stats[\"peak_power_W\"],\n",
    "            \"avg_current_mA\": energy_stats[\"avg_current_mA\"],\n",
    "            \"peak_current_mA\": energy_stats[\"peak_current_mA\"],\n",
    "            \"avg_voltage_V\": energy_stats[\"avg_voltage_V\"],\n",
    "            \"n_samples\": energy_stats[\"n_samples\"],\n",
    "        })\n",
    "    else:\n",
    "        result.update({\n",
    "            \"duration_s\": None,\n",
    "            \"energy_J\": None,\n",
    "            \"avg_power_W\": None,\n",
    "            \"peak_power_W\": None,\n",
    "            \"avg_current_mA\": None,\n",
    "            \"peak_current_mA\": None,\n",
    "            \"avg_voltage_V\": None,\n",
    "            \"n_samples\": 0,\n",
    "        })\n",
    "\n",
    "    # Opcional: print r√°pido\n",
    "    print(f\"{label} -> MAE={_mae:.4f} | RMSE={_rmse:.4f} | R2={_r2:.4f} | \"\n",
    "          f\"t_med={time_per_sample_ms:.3f} ms | E={result['energy_J']} J\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ---------- PR√â-PROCESSAMENTO ----------\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ---------- TREINO (uma √∫nica vez) ----------\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando baseline (sem otimiza√ß√µes) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "\n",
    "    # ---------- 50 RODADAS DE INFER√äNCIA ----------\n",
    "    resultados = []\n",
    "    print(f\"\\n=== Rodando {N_RUNS} infer√™ncias com medi√ß√£o de energia ===\")\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        ts = datetime.now().isoformat(timespec=\"seconds\")\n",
    "        r = evaluate_model_with_energy(model, test_loader, label=f\"Rodada {run:02d}\")\n",
    "        r[\"run\"] = run\n",
    "        r[\"timestamp\"] = ts\n",
    "        resultados.append(r)\n",
    "        time.sleep(SLEEP_BETWEEN_RUNS)  # pequena pausa p/ estabilizar consumo/temperatura\n",
    "\n",
    "    # ---------- SALVAR CSV ----------\n",
    "    df_out = pd.DataFrame(resultados, columns=[\n",
    "        \"run\",\"timestamp\",\n",
    "        \"mae\",\"rmse\",\"r2\",\n",
    "        \"total_inference_s\",\"time_per_sample_ms\",\n",
    "        \"duration_s\",\"energy_J\",\n",
    "        \"avg_power_W\",\"peak_power_W\",\n",
    "        \"avg_current_mA\",\"peak_current_mA\",\n",
    "        \"avg_voltage_V\",\"n_samples\"\n",
    "    ])\n",
    "    df_out.to_csv(OUT_CSV, index=False, float_format=\"%.8f\")\n",
    "    print(f\"\\n‚úÖ Resultados salvos em: {OUT_CSV}\")\n",
    "    print(df_out.describe(include='all'))\n",
    "    # Dica: voc√™ pode depois calcular m√©dias/intervalos de confian√ßa a partir desse CSV.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff36716",
   "metadata": {},
   "source": [
    "## OTIMIZA√á√ïES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b93d19",
   "metadata": {},
   "source": [
    "# Otimiza√ß√£o 1: PODA\n",
    "\n",
    "Vamos implementar 3 diferentes tipos de poda, sendo elas:\n",
    "\n",
    "- Poda L1 n√£o-estruturada (baseline de pruning por magnitude) - com e sem finetune\n",
    "- Poda aleat√≥ria n√£o-estruturada (controle)\n",
    "- Poda estruturada L2 (ln_structured, n=2) em Conv1d (zera filtros/canais inteiros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4c9ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (sem poda) ===\n",
      "Epoch 001/100 - Train MSE: 6.248398\n",
      "Epoch 010/100 - Train MSE: 0.262171\n",
      "Epoch 020/100 - Train MSE: 0.264250\n",
      "Epoch 030/100 - Train MSE: 0.257848\n",
      "Epoch 040/100 - Train MSE: 0.255421\n",
      "Epoch 050/100 - Train MSE: 0.259054\n",
      "Epoch 060/100 - Train MSE: 0.256649\n",
      "Epoch 070/100 - Train MSE: 0.255296\n",
      "Epoch 080/100 - Train MSE: 0.255939\n",
      "Epoch 090/100 - Train MSE: 0.254143\n",
      "Epoch 100/100 - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline (antes da poda):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.174049 s\n",
      "Tempo m√©dio por amostra:   0.0290 ms\n",
      "\n",
      "=== Aplicando poda aleat√≥ria n√£o-estruturada (amount=0.30) ===\n",
      "\n",
      "üîé Sparsity ap√≥s aplicar a poda (reparametrizada):\n",
      "  net.0                     sparsity=0.292\n",
      "  net.2                     sparsity=0.299\n",
      "  net.5                     sparsity=0.300\n",
      "  net.7                     sparsity=0.312\n",
      "\n",
      "üìä Ap√≥s poda aleat√≥ria (sem fine-tuning):\n",
      "MAE:   1.0969\n",
      "RMSE:  2.9628\n",
      "R¬≤:    0.4812\n",
      "Tempo total de infer√™ncia: 0.372575 s\n",
      "Tempo m√©dio por amostra:   0.0620 ms\n",
      "\n",
      "=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\n",
      "\n",
      "üîé Sparsity final (reparam removida):\n",
      "  net.0                     sparsity=0.292\n",
      "  net.2                     sparsity=0.299\n",
      "  net.5                     sparsity=0.300\n",
      "  net.7                     sparsity=0.312\n",
      "\n",
      "üìä Ap√≥s poda aleat√≥ria (reparam removida):\n",
      "MAE:   1.0969\n",
      "RMSE:  2.9628\n",
      "R¬≤:    0.4812\n",
      "Tempo total de infer√™ncia: 0.179909 s\n",
      "Tempo m√©dio por amostra:   0.0299 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_poda_aleatoria_sem_finetune.py\n",
    "# ============================================================\n",
    "# Regress√£o de \"Precipita√ß√£o dia\" com CNN 1D (PyTorch) + poda aleat√≥ria\n",
    "# SEM fine-tuning ap√≥s a poda.\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE = 100\n",
    "LR_BASELINE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "PRUNE_AMOUNT = 0.30  # porcentagem de pesos zerados\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "def layer_sparsity(module: nn.Module) -> float:\n",
    "    w = module.weight.detach().cpu().numpy()\n",
    "    return float((w == 0).mean())\n",
    "\n",
    "def report_sparsity(model: nn.Module, header=\"Sparsity por camada\"):\n",
    "    print(f\"\\nüîé {header}:\")\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            print(f\"  {name:<25s} sparsity={layer_sparsity(m):.3f}\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:03d}/{epochs} - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Poda aleat√≥ria -----------------------\n",
    "def apply_random_unstructured(model: nn.Module, amount: float):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            prune.random_unstructured(m, name=\"weight\", amount=amount)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            prune.random_unstructured(m, name=\"weight\", amount=amount)\n",
    "\n",
    "def remove_pruning_reparam(model: nn.Module):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            if hasattr(m, \"weight_mask\"):\n",
    "                prune.remove(m, \"weight\")\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # Pr√©-processamento\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Baseline\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando baseline (sem poda) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "    evaluate_model(model, test_loader, label=\"Baseline (antes da poda)\")\n",
    "\n",
    "    # Poda aleat√≥ria\n",
    "    print(f\"\\n=== Aplicando poda aleat√≥ria n√£o-estruturada (amount={PRUNE_AMOUNT:.2f}) ===\")\n",
    "    apply_random_unstructured(model, amount=PRUNE_AMOUNT)\n",
    "    report_sparsity(model, header=\"Sparsity ap√≥s aplicar a poda (reparametrizada)\")\n",
    "\n",
    "    # Avalia√ß√£o sem fine-tuning\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda aleat√≥ria (sem fine-tuning)\")\n",
    "\n",
    "    # Remover reparam\n",
    "    print(\"\\n=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\")\n",
    "    remove_pruning_reparam(model)\n",
    "    report_sparsity(model, header=\"Sparsity final (reparam removida)\")\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda aleat√≥ria (reparam removida)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720b80ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (sem poda) ===\n",
      "Epoch 001/100 - Train MSE: 6.248398\n",
      "Epoch 010/100 - Train MSE: 0.262171\n",
      "Epoch 020/100 - Train MSE: 0.264250\n",
      "Epoch 030/100 - Train MSE: 0.257848\n",
      "Epoch 040/100 - Train MSE: 0.255421\n",
      "Epoch 050/100 - Train MSE: 0.259054\n",
      "Epoch 060/100 - Train MSE: 0.256649\n",
      "Epoch 070/100 - Train MSE: 0.255296\n",
      "Epoch 080/100 - Train MSE: 0.255939\n",
      "Epoch 090/100 - Train MSE: 0.254143\n",
      "Epoch 100/100 - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline (antes da poda):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.191164 s\n",
      "Tempo m√©dio por amostra:   0.0318 ms\n",
      "\n",
      "=== Aplicando poda aleat√≥ria n√£o-estruturada (amount=0.30) ===\n",
      "\n",
      "üîé Sparsity ap√≥s aplicar a poda (reparametrizada):\n",
      "  net.0                     sparsity=0.292\n",
      "  net.2                     sparsity=0.299\n",
      "  net.5                     sparsity=0.300\n",
      "  net.7                     sparsity=0.312\n",
      "\n",
      "üìä Ap√≥s poda aleat√≥ria (sem fine-tuning):\n",
      "MAE:   1.0969\n",
      "RMSE:  2.9628\n",
      "R¬≤:    0.4812\n",
      "Tempo total de infer√™ncia: 0.247201 s\n",
      "Tempo m√©dio por amostra:   0.0411 ms\n",
      "\n",
      "=== Fine-tuning p√≥s-poda ===\n",
      "Epoch 001/10 - Train MSE: 1.482756\n",
      "Epoch 010/10 - Train MSE: 0.254416\n",
      "\n",
      "üìä Ap√≥s poda aleat√≥ria + fine-tuning (reparam presente):\n",
      "MAE:   0.1090\n",
      "RMSE:  1.0590\n",
      "R¬≤:    0.9337\n",
      "Tempo total de infer√™ncia: 0.248619 s\n",
      "Tempo m√©dio por amostra:   0.0414 ms\n",
      "\n",
      "=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\n",
      "\n",
      "üîé Sparsity final (reparam removida):\n",
      "  net.0                     sparsity=0.292\n",
      "  net.2                     sparsity=0.299\n",
      "  net.5                     sparsity=0.300\n",
      "  net.7                     sparsity=0.312\n",
      "\n",
      "üìä Ap√≥s poda aleat√≥ria + fine-tuning (reparam removida):\n",
      "MAE:   0.1090\n",
      "RMSE:  1.0590\n",
      "R¬≤:    0.9337\n",
      "Tempo total de infer√™ncia: 0.257712 s\n",
      "Tempo m√©dio por amostra:   0.0429 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_poda_aleatoria_com_finetune.py\n",
    "# ============================================================\n",
    "# Regress√£o \"Precipita√ß√£o dia\" com CNN 1D (PyTorch) + poda aleat√≥ria\n",
    "# COM fine-tuning ap√≥s a poda.\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# ----------------------- Config -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE = 100\n",
    "LR_BASELINE     = 1e-3\n",
    "BATCH_SIZE      = 64\n",
    "SEED            = 42\n",
    "\n",
    "# Poda aleat√≥ria\n",
    "PRUNE_AMOUNT    = 0.30  # % de pesos zerados por camada\n",
    "\n",
    "# Fine-tuning p√≥s-poda\n",
    "EPOCHS_FINETUNE = 10\n",
    "LR_FINETUNE     = 1e-3\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# ----------------------- Utils -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "def layer_sparsity(module: nn.Module) -> float:\n",
    "    w = module.weight.detach().cpu().numpy()\n",
    "    return float((w == 0).mean())\n",
    "\n",
    "def report_sparsity(model: nn.Module, header=\"Sparsity por camada\"):\n",
    "    print(f\"\\nüîé {header}:\")\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            print(f\"  {name:<25s} sparsity={layer_sparsity(m):.3f}\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:03d}/{epochs} - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Poda aleat√≥ria -----------------------\n",
    "def apply_random_unstructured(model: nn.Module, amount: float):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            prune.random_unstructured(m, name=\"weight\", amount=amount)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            prune.random_unstructured(m, name=\"weight\", amount=amount)\n",
    "\n",
    "def remove_pruning_reparam(model: nn.Module):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            if hasattr(m, \"weight_mask\"):\n",
    "                prune.remove(m, \"weight\")\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # Pr√©-processamento (igual ao seu)\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Baseline\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando baseline (sem poda) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "    evaluate_model(model, test_loader, label=\"Baseline (antes da poda)\")\n",
    "\n",
    "    # Poda aleat√≥ria (impacto bruto)\n",
    "    print(f\"\\n=== Aplicando poda aleat√≥ria n√£o-estruturada (amount={PRUNE_AMOUNT:.2f}) ===\")\n",
    "    apply_random_unstructured(model, amount=PRUNE_AMOUNT)\n",
    "    report_sparsity(model, header=\"Sparsity ap√≥s aplicar a poda (reparametrizada)\")\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda aleat√≥ria (sem fine-tuning)\")\n",
    "\n",
    "    # Fine-tuning p√≥s-poda\n",
    "    print(\"\\n=== Fine-tuning p√≥s-poda ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_FINETUNE, lr=LR_FINETUNE)\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda aleat√≥ria + fine-tuning (reparam presente)\")\n",
    "\n",
    "    # Remover reparam e reavaliar\n",
    "    print(\"\\n=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\")\n",
    "    remove_pruning_reparam(model)\n",
    "    report_sparsity(model, header=\"Sparsity final (reparam removida)\")\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda aleat√≥ria + fine-tuning (reparam removida)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80caed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (sem poda) ===\n",
      "Epoch 001/100  - Train MSE: 6.248398\n",
      "Epoch 010/100  - Train MSE: 0.262171\n",
      "Epoch 020/100  - Train MSE: 0.264250\n",
      "Epoch 030/100  - Train MSE: 0.257848\n",
      "Epoch 040/100  - Train MSE: 0.255421\n",
      "Epoch 050/100  - Train MSE: 0.259054\n",
      "Epoch 060/100  - Train MSE: 0.256649\n",
      "Epoch 070/100  - Train MSE: 0.255296\n",
      "Epoch 080/100  - Train MSE: 0.255939\n",
      "Epoch 090/100  - Train MSE: 0.254143\n",
      "Epoch 100/100  - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline (antes da poda):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.258988 s\n",
      "Tempo m√©dio por amostra:   0.0431 ms\n",
      "\n",
      "=== Aplicando poda L1 n√£o-estruturada (amount=0.30) ===\n",
      "\n",
      "üîé Sparsity ap√≥s aplicar a poda (reparametrizada):\n",
      "  net.0                     sparsity=0.292\n",
      "  net.2                     sparsity=0.299\n",
      "  net.5                     sparsity=0.300\n",
      "  net.7                     sparsity=0.312\n",
      "\n",
      "üìä Ap√≥s poda L1 (sem fine-tuning):\n",
      "MAE:   0.1807\n",
      "RMSE:  1.0713\n",
      "R¬≤:    0.9322\n",
      "Tempo total de infer√™ncia: 0.222869 s\n",
      "Tempo m√©dio por amostra:   0.0371 ms\n",
      "\n",
      "=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\n",
      "\n",
      "üîé Sparsity final (reparam removida):\n",
      "  net.0                     sparsity=0.292\n",
      "  net.2                     sparsity=0.299\n",
      "  net.5                     sparsity=0.300\n",
      "  net.7                     sparsity=0.312\n",
      "\n",
      "üìä Ap√≥s poda L1 (reparam removida, sem fine-tuning):\n",
      "MAE:   0.1807\n",
      "RMSE:  1.0713\n",
      "R¬≤:    0.9322\n",
      "Tempo total de infer√™ncia: 0.197637 s\n",
      "Tempo m√©dio por amostra:   0.0329 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_poda_l1_sem_finetune.py\n",
    "# ============================================================\n",
    "# Regress√£o de \"Precipita√ß√£o dia\" com CNN 1D (PyTorch) + poda L1\n",
    "# SEM fine-tuning ap√≥s a poda.\n",
    "# Pr√©-processamento exatamente como especificado.\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE = 100        # <- 100 √©pocas, conforme pedido\n",
    "LR_BASELINE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "# For√ßar CPU\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "def layer_sparsity(module: nn.Module) -> float:\n",
    "    w = module.weight.detach().cpu().numpy()\n",
    "    return float((w == 0).mean())\n",
    "\n",
    "def report_sparsity(model: nn.Module, header=\"Sparsity por camada\"):\n",
    "    print(f\"\\nüîé {header}:\")\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            print(f\"  {name:<25s} sparsity={layer_sparsity(m):.3f}\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 16*length)\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)    # (N,)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)            # CPU direto\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:03d}/{epochs}  - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Poda L1 (n√£o-estruturada) -----------------------\n",
    "def apply_l1_unstructured(model: nn.Module, amount: float):\n",
    "    # Aplica L1 n√£o-estruturada em Conv1d e Linear (apenas nos pesos)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            prune.l1_unstructured(m, name=\"weight\", amount=amount)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            prune.l1_unstructured(m, name=\"weight\", amount=amount)\n",
    "\n",
    "def remove_pruning_reparam(model: nn.Module):\n",
    "    # Remove reparametriza√ß√£o (weight_orig/weight_mask) e fixa zeros permanentemente\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            if hasattr(m, \"weight_mask\"):\n",
    "                prune.remove(m, \"weight\")\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ============================================================\n",
    "    # PR√â-PROCESSAMENTO (exatamente como informado)\n",
    "    # ============================================================\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # BASELINE: treino e avalia√ß√£o ANTES da poda\n",
    "    # ============================================================\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "\n",
    "    print(\"\\n=== Treinando baseline (sem poda) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "    evaluate_model(model, test_loader, label=\"Baseline (antes da poda)\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Poda L1 n√£o-estruturada (SEM fine-tuning)\n",
    "    # ============================================================\n",
    "    PRUNE_AMOUNT = 0.30  # ajuste aqui se quiser testar outros n√≠veis\n",
    "    print(f\"\\n=== Aplicando poda L1 n√£o-estruturada (amount={PRUNE_AMOUNT:.2f}) ===\")\n",
    "    apply_l1_unstructured(model, amount=PRUNE_AMOUNT)\n",
    "    report_sparsity(model, header=\"Sparsity ap√≥s aplicar a poda (reparametrizada)\")\n",
    "\n",
    "    # Avalia√ß√£o imediatamente ap√≥s a poda (sem treinar de novo)\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L1 (sem fine-tuning)\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Remover reparametriza√ß√£o e reavaliar\n",
    "    # ============================================================\n",
    "    print(\"\\n=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\")\n",
    "    remove_pruning_reparam(model)\n",
    "    report_sparsity(model, header=\"Sparsity final (reparam removida)\")\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L1 (reparam removida, sem fine-tuning)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a52bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (sem poda) ===\n",
      "Epoch 001/100  - Train MSE: 6.248398\n",
      "Epoch 010/100  - Train MSE: 0.262171\n",
      "Epoch 020/100  - Train MSE: 0.264250\n",
      "Epoch 030/100  - Train MSE: 0.257848\n",
      "Epoch 040/100  - Train MSE: 0.255421\n",
      "Epoch 050/100  - Train MSE: 0.259054\n",
      "Epoch 060/100  - Train MSE: 0.256649\n",
      "Epoch 070/100  - Train MSE: 0.255296\n",
      "Epoch 080/100  - Train MSE: 0.255939\n",
      "Epoch 090/100  - Train MSE: 0.254143\n",
      "Epoch 100/100  - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline (antes da poda):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.175219 s\n",
      "Tempo m√©dio por amostra:   0.0292 ms\n",
      "\n",
      "=== Aplicando poda L1 n√£o-estruturada (amount=0.30) ===\n",
      "\n",
      "üîé Sparsity ap√≥s aplicar a poda (reparametrizada):\n",
      "  net.0                     sparsity=0.292\n",
      "  net.2                     sparsity=0.299\n",
      "  net.5                     sparsity=0.300\n",
      "  net.7                     sparsity=0.312\n",
      "\n",
      "üìä Ap√≥s poda L1 (sem fine-tuning):\n",
      "MAE:   0.1807\n",
      "RMSE:  1.0713\n",
      "R¬≤:    0.9322\n",
      "Tempo total de infer√™ncia: 0.194088 s\n",
      "Tempo m√©dio por amostra:   0.0323 ms\n",
      "\n",
      "=== Fine-tuning p√≥s-poda ===\n",
      "Epoch 001/10  - Train MSE: 0.251069\n",
      "Epoch 010/10  - Train MSE: 0.249120\n",
      "\n",
      "üìä Ap√≥s poda L1 + fine-tuning (reparam presente):\n",
      "MAE:   0.1128\n",
      "RMSE:  1.0596\n",
      "R¬≤:    0.9336\n",
      "Tempo total de infer√™ncia: 0.186872 s\n",
      "Tempo m√©dio por amostra:   0.0311 ms\n",
      "\n",
      "=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\n",
      "\n",
      "üîé Sparsity final (reparam removida):\n",
      "  net.0                     sparsity=0.292\n",
      "  net.2                     sparsity=0.299\n",
      "  net.5                     sparsity=0.300\n",
      "  net.7                     sparsity=0.312\n",
      "\n",
      "üìä Ap√≥s poda L1 + fine-tuning (reparam removida):\n",
      "MAE:   0.1128\n",
      "RMSE:  1.0596\n",
      "R¬≤:    0.9336\n",
      "Tempo total de infer√™ncia: 0.201933 s\n",
      "Tempo m√©dio por amostra:   0.0336 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_poda_l1_com_finetune.py\n",
    "# ============================================================\n",
    "# Regress√£o de \"Precipita√ß√£o dia\" com CNN 1D (PyTorch) + poda L1\n",
    "# COM fine-tuning ap√≥s a poda.\n",
    "# Pr√©-processamento exatamente como especificado.\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE  = 100       # treino inicial (baseline)\n",
    "LR_BASELINE      = 1e-3\n",
    "BATCH_SIZE       = 64\n",
    "SEED             = 42\n",
    "\n",
    "# Poda L1\n",
    "PRUNE_AMOUNT     = 0.30      # 30% dos pesos zerados em cada camada Conv/Linear\n",
    "\n",
    "# Fine-tuning p√≥s-poda\n",
    "EPOCHS_FINETUNE  = 10        # ajuste aqui se quiser\n",
    "LR_FINETUNE      = 1e-3\n",
    "\n",
    "# For√ßar CPU\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "def layer_sparsity(module: nn.Module) -> float:\n",
    "    w = module.weight.detach().cpu().numpy()\n",
    "    return float((w == 0).mean())\n",
    "\n",
    "def report_sparsity(model: nn.Module, header=\"Sparsity por camada\"):\n",
    "    print(f\"\\nüîé {header}:\")\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            print(f\"  {name:<25s} sparsity={layer_sparsity(m):.3f}\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 16*length)\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)    # (N,)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)            # CPU direto\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:03d}/{epochs}  - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Poda L1 (n√£o-estruturada) -----------------------\n",
    "def apply_l1_unstructured(model: nn.Module, amount: float):\n",
    "    # Aplica L1 n√£o-estruturada em Conv1d e Linear (apenas nos pesos)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            prune.l1_unstructured(m, name=\"weight\", amount=amount)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            prune.l1_unstructured(m, name=\"weight\", amount=amount)\n",
    "\n",
    "def remove_pruning_reparam(model: nn.Module):\n",
    "    # Remove reparametriza√ß√£o (weight_orig/weight_mask) e fixa zeros permanentemente\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            if hasattr(m, \"weight_mask\"):\n",
    "                prune.remove(m, \"weight\")\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ============================================================\n",
    "    # PR√â-PROCESSAMENTO (exatamente como informado)\n",
    "    # ============================================================\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # BASELINE: treino e avalia√ß√£o ANTES da poda\n",
    "    # ============================================================\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "\n",
    "    print(\"\\n=== Treinando baseline (sem poda) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "    evaluate_model(model, test_loader, label=\"Baseline (antes da poda)\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Poda L1 n√£o-estruturada\n",
    "    # ============================================================\n",
    "    print(f\"\\n=== Aplicando poda L1 n√£o-estruturada (amount={PRUNE_AMOUNT:.2f}) ===\")\n",
    "    apply_l1_unstructured(model, amount=PRUNE_AMOUNT)\n",
    "    report_sparsity(model, header=\"Sparsity ap√≥s aplicar a poda (reparametrizada)\")\n",
    "\n",
    "    # Avaliar imediatamente ap√≥s a poda (sem treino) ‚Äî impacto \"bruto\"\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L1 (sem fine-tuning)\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Fine-tuning p√≥s-poda\n",
    "    # ============================================================\n",
    "    print(\"\\n=== Fine-tuning p√≥s-poda ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_FINETUNE, lr=LR_FINETUNE)\n",
    "\n",
    "    # Avaliar ap√≥s fine-tune (ainda com reparam)\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L1 + fine-tuning (reparam presente)\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Remover reparametriza√ß√£o e reavaliar\n",
    "    # ============================================================\n",
    "    print(\"\\n=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\")\n",
    "    remove_pruning_reparam(model)\n",
    "    report_sparsity(model, header=\"Sparsity final (reparam removida)\")\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L1 + fine-tuning (reparam removida)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58776aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (sem poda) ===\n",
      "Epoch 001/100 - Train MSE: 6.248398\n",
      "Epoch 010/100 - Train MSE: 0.262171\n",
      "Epoch 020/100 - Train MSE: 0.264250\n",
      "Epoch 030/100 - Train MSE: 0.257848\n",
      "Epoch 040/100 - Train MSE: 0.255421\n",
      "Epoch 050/100 - Train MSE: 0.259054\n",
      "Epoch 060/100 - Train MSE: 0.256649\n",
      "Epoch 070/100 - Train MSE: 0.255296\n",
      "Epoch 080/100 - Train MSE: 0.255939\n",
      "Epoch 090/100 - Train MSE: 0.254143\n",
      "Epoch 100/100 - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline (antes da poda):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.450163 s\n",
      "Tempo m√©dio por amostra:   0.0749 ms\n",
      "\n",
      "=== Aplicando poda estruturada L2 (amount=0.30, n=2, dim=0) ===\n",
      "\n",
      "üîé Sparsity ap√≥s aplicar a poda (reparametrizada):\n",
      "  net.0                     sparsity=0.250\n",
      "  net.2                     sparsity=0.312\n",
      "  net.5                     sparsity=0.312\n",
      "  net.7                     sparsity=0.000\n",
      "\n",
      "üìä Ap√≥s poda L2 estruturada (sem fine-tuning):\n",
      "MAE:   0.8318\n",
      "RMSE:  1.9054\n",
      "R¬≤:    0.7854\n",
      "Tempo total de infer√™ncia: 0.611392 s\n",
      "Tempo m√©dio por amostra:   0.1017 ms\n",
      "\n",
      "=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\n",
      "\n",
      "üîé Sparsity final (reparam removida):\n",
      "  net.0                     sparsity=0.250\n",
      "  net.2                     sparsity=0.312\n",
      "  net.5                     sparsity=0.312\n",
      "  net.7                     sparsity=0.000\n",
      "\n",
      "üìä Ap√≥s poda L2 estruturada (reparam removida):\n",
      "MAE:   0.8318\n",
      "RMSE:  1.9054\n",
      "R¬≤:    0.7854\n",
      "Tempo total de infer√™ncia: 0.223068 s\n",
      "Tempo m√©dio por amostra:   0.0371 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_poda_l2_estruturada_sem_finetune.py\n",
    "# ============================================================\n",
    "# Regress√£o \"Precipita√ß√£o dia\" com CNN 1D (PyTorch)\n",
    "# Poda estruturada L2 (ln_structured, n=2) SEM fine-tuning\n",
    "# - Conv1d: zera filtros (dim=0)\n",
    "# - Linear: zera neur√¥nios de sa√≠da (dim=0)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# ----------------------- Config -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE = 100\n",
    "LR_BASELINE     = 1e-3\n",
    "BATCH_SIZE      = 64\n",
    "SEED            = 42\n",
    "\n",
    "# Poda estruturada L2\n",
    "PRUNE_AMOUNT    = 0.30   # propor√ß√£o de filtros/neur√¥nios a serem zerados por camada\n",
    "N_NORM          = 2      # L2\n",
    "DIM_CONV        = 0      # prune filtros (out_channels)\n",
    "DIM_LINEAR      = 0      # prune neur√¥nios de sa√≠da (out_features)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# ----------------------- Utils -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "def layer_sparsity(module: nn.Module) -> float:\n",
    "    w = module.weight.detach().cpu().numpy()\n",
    "    return float((w == 0).mean())\n",
    "\n",
    "def report_sparsity(model: nn.Module, header=\"Sparsity por camada\"):\n",
    "    print(f\"\\nüîé {header}:\")\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            print(f\"  {name:<25s} sparsity={layer_sparsity(m):.3f}\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 16*length)\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:03d}/{epochs} - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Poda estruturada L2 -----------------------\n",
    "def apply_l2_structured(model: nn.Module, amount: float, n_norm: int = 2):\n",
    "    # Conv1d: prune filtros inteiros (dim=0)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            prune.ln_structured(m, name=\"weight\", amount=amount, n=n_norm, dim=DIM_CONV)\n",
    "    # Linear: prune neur√¥nios de sa√≠da inteiros (dim=0)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            prune.ln_structured(m, name=\"weight\", amount=amount, n=n_norm, dim=DIM_LINEAR)\n",
    "\n",
    "def remove_pruning_reparam(model: nn.Module):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            if hasattr(m, \"weight_mask\"):\n",
    "                prune.remove(m, \"weight\")\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # Pr√©-processamento\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Baseline\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando baseline (sem poda) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "    evaluate_model(model, test_loader, label=\"Baseline (antes da poda)\")\n",
    "\n",
    "    # Poda L2 estruturada (impacto bruto)\n",
    "    print(f\"\\n=== Aplicando poda estruturada L2 (amount={PRUNE_AMOUNT:.2f}, n={N_NORM}, dim=0) ===\")\n",
    "    apply_l2_structured(model, amount=PRUNE_AMOUNT, n_norm=N_NORM)\n",
    "    report_sparsity(model, header=\"Sparsity ap√≥s aplicar a poda (reparametrizada)\")\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L2 estruturada (sem fine-tuning)\")\n",
    "\n",
    "    # Remover reparam e reavaliar\n",
    "    print(\"\\n=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\")\n",
    "    remove_pruning_reparam(model)\n",
    "    report_sparsity(model, header=\"Sparsity final (reparam removida)\")\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L2 estruturada (reparam removida)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e4cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (sem poda) ===\n",
      "Epoch 001/100 - Train MSE: 6.248398\n",
      "Epoch 010/100 - Train MSE: 0.262171\n",
      "Epoch 020/100 - Train MSE: 0.264250\n",
      "Epoch 030/100 - Train MSE: 0.257848\n",
      "Epoch 040/100 - Train MSE: 0.255421\n",
      "Epoch 050/100 - Train MSE: 0.259054\n",
      "Epoch 060/100 - Train MSE: 0.256649\n",
      "Epoch 070/100 - Train MSE: 0.255296\n",
      "Epoch 080/100 - Train MSE: 0.255939\n",
      "Epoch 090/100 - Train MSE: 0.254143\n",
      "Epoch 100/100 - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline (antes da poda):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.466527 s\n",
      "Tempo m√©dio por amostra:   0.0776 ms\n",
      "\n",
      "=== Aplicando poda estruturada L2 (amount=0.30, n=2, dim=0) ===\n",
      "\n",
      "üîé Sparsity ap√≥s aplicar a poda (reparametrizada):\n",
      "  net.0                     sparsity=0.250\n",
      "  net.2                     sparsity=0.312\n",
      "  net.5                     sparsity=0.312\n",
      "  net.7                     sparsity=0.000\n",
      "\n",
      "üìä Ap√≥s poda L2 estruturada (sem fine-tuning):\n",
      "MAE:   0.8318\n",
      "RMSE:  1.9054\n",
      "R¬≤:    0.7854\n",
      "Tempo total de infer√™ncia: 0.625842 s\n",
      "Tempo m√©dio por amostra:   0.1042 ms\n",
      "\n",
      "=== Fine-tuning p√≥s-poda L2 estruturada ===\n",
      "Epoch 001/10 - Train MSE: 0.452478\n",
      "Epoch 010/10 - Train MSE: 0.250547\n",
      "\n",
      "üìä Ap√≥s poda L2 estruturada + fine-tuning (reparam presente):\n",
      "MAE:   0.1402\n",
      "RMSE:  1.0641\n",
      "R¬≤:    0.9331\n",
      "Tempo total de infer√™ncia: 0.333496 s\n",
      "Tempo m√©dio por amostra:   0.0555 ms\n",
      "\n",
      "=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\n",
      "\n",
      "üîé Sparsity final (reparam removida):\n",
      "  net.0                     sparsity=0.250\n",
      "  net.2                     sparsity=0.312\n",
      "  net.5                     sparsity=0.312\n",
      "  net.7                     sparsity=0.000\n",
      "\n",
      "üìä Ap√≥s poda L2 estruturada + fine-tuning (reparam removida):\n",
      "MAE:   0.1402\n",
      "RMSE:  1.0641\n",
      "R¬≤:    0.9331\n",
      "Tempo total de infer√™ncia: 0.410719 s\n",
      "Tempo m√©dio por amostra:   0.0684 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_poda_l2_estruturada_com_finetune.py\n",
    "# ============================================================\n",
    "# Regress√£o \"Precipita√ß√£o dia\" com CNN 1D (PyTorch)\n",
    "# Poda estruturada L2 (ln_structured, n=2) COM fine-tuning\n",
    "# - Conv1d: zera filtros (dim=0)\n",
    "# - Linear: zera neur√¥nios de sa√≠da (dim=0)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# ----------------------- Config -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE  = 100\n",
    "LR_BASELINE      = 1e-3\n",
    "BATCH_SIZE       = 64\n",
    "SEED             = 42\n",
    "\n",
    "# Poda estruturada L2\n",
    "PRUNE_AMOUNT     = 0.30   # propor√ß√£o de filtros/neur√¥nios a zerar por camada\n",
    "N_NORM           = 2      # L2\n",
    "DIM_CONV         = 0      # out_channels (filtros)\n",
    "DIM_LINEAR       = 0      # out_features (neur√¥nios de sa√≠da)\n",
    "\n",
    "# Fine-tuning p√≥s-poda (mesmo esquema para todas as podas)\n",
    "EPOCHS_FINETUNE  = 10\n",
    "LR_FINETUNE      = 1e-3\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# ----------------------- Utils -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "def layer_sparsity(module: nn.Module) -> float:\n",
    "    w = module.weight.detach().cpu().numpy()\n",
    "    return float((w == 0).mean())\n",
    "\n",
    "def report_sparsity(model: nn.Module, header=\"Sparsity por camada\"):\n",
    "    print(f\"\\nüîé {header}:\")\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            print(f\"  {name:<25s} sparsity={layer_sparsity(m):.3f}\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 16*length)\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:03d}/{epochs} - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Poda L2 estruturada -----------------------\n",
    "def apply_l2_structured(model: nn.Module, amount: float, n_norm: int = 2):\n",
    "    # Conv1d: zera filtros (dim=0)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            prune.ln_structured(m, name=\"weight\", amount=amount, n=n_norm, dim=DIM_CONV)\n",
    "    # Linear: zera neur√¥nios de sa√≠da (dim=0)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            prune.ln_structured(m, name=\"weight\", amount=amount, n=n_norm, dim=DIM_LINEAR)\n",
    "\n",
    "def remove_pruning_reparam(model: nn.Module):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            if hasattr(m, \"weight_mask\"):\n",
    "                prune.remove(m, \"weight\")\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # Pr√©-processamento (igual aos demais scripts)\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Baseline\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando baseline (sem poda) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "    evaluate_model(model, test_loader, label=\"Baseline (antes da poda)\")\n",
    "\n",
    "    # Poda estruturada L2 (impacto bruto)\n",
    "    print(f\"\\n=== Aplicando poda estruturada L2 (amount={PRUNE_AMOUNT:.2f}, n={N_NORM}, dim=0) ===\")\n",
    "    apply_l2_structured(model, amount=PRUNE_AMOUNT, n_norm=N_NORM)\n",
    "    report_sparsity(model, header=\"Sparsity ap√≥s aplicar a poda (reparametrizada)\")\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L2 estruturada (sem fine-tuning)\")\n",
    "\n",
    "    # Fine-tuning p√≥s-poda (mesma config usada nas demais podas)\n",
    "    print(\"\\n=== Fine-tuning p√≥s-poda L2 estruturada ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_FINETUNE, lr=LR_FINETUNE)\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L2 estruturada + fine-tuning (reparam presente)\")\n",
    "\n",
    "    # Remover reparam e reavaliar\n",
    "    print(\"\\n=== Removendo reparametriza√ß√£o da poda (zeros permanentes) ===\")\n",
    "    remove_pruning_reparam(model)\n",
    "    report_sparsity(model, header=\"Sparsity final (reparam removida)\")\n",
    "    evaluate_model(model, test_loader, label=\"Ap√≥s poda L2 estruturada + fine-tuning (reparam removida)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relat√≥rio das podas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3fd45",
   "metadata": {},
   "source": [
    "# Otimiza√ß√£o 2: Quantiza√ß√£o\n",
    "\n",
    "- Quantiza√ß√£o din√¢mica\n",
    "- Quantiza√ß√£o Est√°tica p√≥s-treino\n",
    "- Quantiza√ß√£o Consciente de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6560f235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (sem otimiza√ß√µes) ===\n",
      "Epoch 01/100  - Train MSE: 6.248398\n",
      "Epoch 05/100  - Train MSE: 0.270467\n",
      "Epoch 10/100  - Train MSE: 0.262171\n",
      "Epoch 15/100  - Train MSE: 0.258560\n",
      "Epoch 20/100  - Train MSE: 0.264250\n",
      "Epoch 25/100  - Train MSE: 0.264546\n",
      "Epoch 30/100  - Train MSE: 0.257848\n",
      "Epoch 35/100  - Train MSE: 0.268403\n",
      "Epoch 40/100  - Train MSE: 0.255421\n",
      "Epoch 45/100  - Train MSE: 0.264791\n",
      "Epoch 50/100  - Train MSE: 0.259054\n",
      "Epoch 55/100  - Train MSE: 0.253974\n",
      "Epoch 60/100  - Train MSE: 0.256649\n",
      "Epoch 65/100  - Train MSE: 0.261080\n",
      "Epoch 70/100  - Train MSE: 0.255296\n",
      "Epoch 75/100  - Train MSE: 0.252065\n",
      "Epoch 80/100  - Train MSE: 0.255939\n",
      "Epoch 85/100  - Train MSE: 0.253428\n",
      "Epoch 90/100  - Train MSE: 0.254143\n",
      "Epoch 95/100  - Train MSE: 0.256427\n",
      "Epoch 100/100  - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline (sem otimiza√ß√µes):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.127923 s\n",
      "Tempo m√©dio por amostra:   0.0213 ms\n",
      "\n",
      "üìä Quantiza√ß√£o Din√¢mica (INT8):\n",
      "MAE:   0.1428\n",
      "RMSE:  1.0635\n",
      "R¬≤:    0.9331\n",
      "Tempo total de infer√™ncia: 0.225782 s\n",
      "Tempo m√©dio por amostra:   0.0376 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_baseline.py\n",
    "# ============================================================\n",
    "# Regress√£o de \"Precipita√ß√£o dia\" com CNN 1D (PyTorch) - BASELINE\n",
    "# Com SEED e LR_BASELINE para reprodutibilidade.\n",
    "# Sem poda / quantiza√ß√£o / qualquer otimiza√ß√£o.\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>> ADI√á√ÉO <<<<<<<<<<<<<<<<<<<<<<<\n",
    "import torch.ao.quantization as quant  # para quantiza√ß√£o din√¢mica\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE = 100\n",
    "LR_BASELINE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # for√ßado CPU, como voc√™ pediu\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 16*length)\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)    # (N,)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)            # CPU direto\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs}  - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ============================================================\n",
    "    # PR√â-PROCESSAMENTO (exatamente como voc√™ definiu)\n",
    "    # ============================================================\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # BASELINE: treino e avalia√ß√£o\n",
    "    # ============================================================\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "\n",
    "    print(\"\\n=== Treinando baseline (sem otimiza√ß√µes) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "    evaluate_model(model, test_loader, label=\"Baseline (sem otimiza√ß√µes)\")\n",
    "\n",
    "    # ============================================================\n",
    "    # QUANTIZA√á√ÉO DIN√ÇMICA (apenas Linear) - m√≠nima mudan√ßa\n",
    "    # ============================================================\n",
    "    model.eval()\n",
    "    qmodel = quant.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},          # quantiza apenas camadas Lineares\n",
    "        dtype=torch.qint8     # pesos INT8; ativa√ß√µes permanecem em float\n",
    "    )\n",
    "    evaluate_model(qmodel, test_loader, label=\"Quantiza√ß√£o Din√¢mica (INT8)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86caa28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (sem otimiza√ß√µes) ===\n",
      "Epoch 01/100  - Train MSE: 6.248398\n",
      "Epoch 05/100  - Train MSE: 0.270467\n",
      "Epoch 10/100  - Train MSE: 0.262171\n",
      "Epoch 15/100  - Train MSE: 0.258560\n",
      "Epoch 20/100  - Train MSE: 0.264250\n",
      "Epoch 25/100  - Train MSE: 0.264546\n",
      "Epoch 30/100  - Train MSE: 0.257848\n",
      "Epoch 35/100  - Train MSE: 0.268403\n",
      "Epoch 40/100  - Train MSE: 0.255421\n",
      "Epoch 45/100  - Train MSE: 0.264791\n",
      "Epoch 50/100  - Train MSE: 0.259054\n",
      "Epoch 55/100  - Train MSE: 0.253974\n",
      "Epoch 60/100  - Train MSE: 0.256649\n",
      "Epoch 65/100  - Train MSE: 0.261080\n",
      "Epoch 70/100  - Train MSE: 0.255296\n",
      "Epoch 75/100  - Train MSE: 0.252065\n",
      "Epoch 80/100  - Train MSE: 0.255939\n",
      "Epoch 85/100  - Train MSE: 0.253428\n",
      "Epoch 90/100  - Train MSE: 0.254143\n",
      "Epoch 95/100  - Train MSE: 0.256427\n",
      "Epoch 100/100  - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline (sem otimiza√ß√µes):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.171769 s\n",
      "Tempo m√©dio por amostra:   0.0286 ms\n",
      "\n",
      "=== PTQ Est√°tica (INT8) ===\n",
      "Backend de quantiza√ß√£o: fbgemm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clariele/Desktop/mestrado/Qualificacao/venv_cnn_v1/lib/python3.8/site-packages/torch/ao/quantization/observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä PTQ Est√°tica (INT8, engine auto):\n",
      "MAE:   0.1423\n",
      "RMSE:  1.0706\n",
      "R¬≤:    0.9322\n",
      "Tempo total de infer√™ncia: 0.047312 s\n",
      "Tempo m√©dio por amostra:   0.0079 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_baseline.py\n",
    "# ============================================================\n",
    "# Regress√£o de \"Precipita√ß√£o dia\" com CNN 1D (PyTorch) - BASELINE\n",
    "# + PTQ Est√°tica (INT8) com backend auto (fbgemm/qnnpack) e √∫ltima camada em float\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# >>> PTQ est√°tica (FX) - imports\n",
    "from torch.ao.quantization import get_default_qconfig\n",
    "try:\n",
    "    from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "except ImportError:\n",
    "    from torch.ao.quantization.fx import prepare_fx, convert_fx  # type: ignore\n",
    "from torch.ao.quantization.qconfig_mapping import QConfigMapping\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE = 100\n",
    "LR_BASELINE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # for√ßado CPU\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 16*length)\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)             # <- manter em float na PTQ\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)    # (N,)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)            # CPU direto\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs}  - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ============================================================\n",
    "    # PR√â-PROCESSAMENTO (igual ao baseline)\n",
    "    # ============================================================\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # BASELINE: treino e avalia√ß√£o\n",
    "    # ============================================================\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "\n",
    "    print(\"\\n=== Treinando baseline (sem otimiza√ß√µes) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "    evaluate_model(model, test_loader, label=\"Baseline (sem otimiza√ß√µes)\")\n",
    "\n",
    "    # ============================================================\n",
    "    # PTQ EST√ÅTICA (INT8) ‚Äî engine auto + QConfigMapping\n",
    "    # ============================================================\n",
    "    print(\"\\n=== PTQ Est√°tica (INT8) ===\")\n",
    "    engines = torch.backends.quantized.supported_engines\n",
    "    engine = \"fbgemm\" if \"fbgemm\" in engines else (\"qnnpack\" if \"qnnpack\" in engines else None)\n",
    "    if engine is None:\n",
    "        raise RuntimeError(f\"Nenhum backend de quantiza√ß√£o suportado encontrado: {engines}\")\n",
    "    torch.backends.quantized.engine = engine\n",
    "    print(f\"Backend de quantiza√ß√£o: {engine}\")\n",
    "\n",
    "    example_input = torch.randn(1, 1, X_train_cnn.shape[2])\n",
    "\n",
    "    qconfig = get_default_qconfig(engine)\n",
    "    qconfig_mapping = (\n",
    "        QConfigMapping()\n",
    "        .set_global(qconfig)              # aplica a todos\n",
    "        .set_module_name(\"net.7\", None)   # N√ÉO quantizar a √∫ltima Linear\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    prepared = prepare_fx(model, qconfig_mapping, example_inputs=example_input)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for xb, _ in train_loader:\n",
    "            prepared(xb)\n",
    "\n",
    "    quantized_model = convert_fx(prepared).to(DEVICE).eval()\n",
    "\n",
    "    evaluate_model(quantized_model, test_loader, label=\"PTQ Est√°tica (INT8, engine auto)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dae5b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando baseline (FP32) ===\n",
      "Epoch 01/100  - Train MSE: 6.248398\n",
      "Epoch 05/100  - Train MSE: 0.270467\n",
      "Epoch 10/100  - Train MSE: 0.262171\n",
      "Epoch 15/100  - Train MSE: 0.258560\n",
      "Epoch 20/100  - Train MSE: 0.264250\n",
      "Epoch 25/100  - Train MSE: 0.264546\n",
      "Epoch 30/100  - Train MSE: 0.257848\n",
      "Epoch 35/100  - Train MSE: 0.268403\n",
      "Epoch 40/100  - Train MSE: 0.255421\n",
      "Epoch 45/100  - Train MSE: 0.264791\n",
      "Epoch 50/100  - Train MSE: 0.259054\n",
      "Epoch 55/100  - Train MSE: 0.253974\n",
      "Epoch 60/100  - Train MSE: 0.256649\n",
      "Epoch 65/100  - Train MSE: 0.261080\n",
      "Epoch 70/100  - Train MSE: 0.255296\n",
      "Epoch 75/100  - Train MSE: 0.252065\n",
      "Epoch 80/100  - Train MSE: 0.255939\n",
      "Epoch 85/100  - Train MSE: 0.253428\n",
      "Epoch 90/100  - Train MSE: 0.254143\n",
      "Epoch 95/100  - Train MSE: 0.256427\n",
      "Epoch 100/100  - Train MSE: 0.253061\n",
      "\n",
      "üìä Baseline FP32:\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.159183 s\n",
      "Tempo m√©dio por amostra:   0.0265 ms\n",
      "\n",
      "=== QAT (fake-quant) + Convers√£o INT8 ===\n",
      "Backend de quantiza√ß√£o: fbgemm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clariele/Desktop/mestrado/Qualificacao/venv_cnn_v1/lib/python3.8/site-packages/torch/ao/quantization/observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QAT] Epoch 01/15 - Train MSE: 2.920815\n",
      "[QAT] Epoch 02/15 - Train MSE: 3.455787\n",
      "[QAT] Epoch 03/15 - Train MSE: 3.481976\n",
      "[QAT] Epoch 04/15 - Train MSE: 3.542840\n",
      "[QAT] Epoch 05/15 - Train MSE: 3.610246\n",
      "[QAT] Epoch 06/15 - Train MSE: 3.696140\n",
      "[QAT] Epoch 07/15 - Train MSE: 3.468919\n",
      "[QAT] Epoch 08/15 - Train MSE: 3.490423\n",
      "[QAT] Epoch 09/15 - Train MSE: 3.440322\n",
      "[QAT] Epoch 10/15 - Train MSE: 3.646849\n",
      "[QAT] Epoch 11/15 - Train MSE: 3.684465\n",
      "[QAT] Epoch 12/15 - Train MSE: 3.456197\n",
      "[QAT] Epoch 13/15 - Train MSE: 3.544619\n",
      "[QAT] Epoch 14/15 - Train MSE: 3.661553\n",
      "[QAT] Epoch 15/15 - Train MSE: 3.564728\n",
      "\n",
      "üìä QAT ‚Üí INT8 (fix):\n",
      "MAE:   0.1268\n",
      "RMSE:  1.2724\n",
      "R¬≤:    0.9043\n",
      "Tempo total de infer√™ncia: 0.117162 s\n",
      "Tempo m√©dio por amostra:   0.0195 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_qat.py\n",
    "# ============================================================\n",
    "# Regress√£o \"Precipita√ß√£o dia\" com CNN 1D (PyTorch) - QAT ONLY\n",
    "# - Backend auto (fbgemm/qnnpack)\n",
    "# - √öltima camada em float\n",
    "# - Fine-tuning curto com fake-quant + convers√£o para INT8\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# >>> QAT (FX) - imports\n",
    "from torch.ao.quantization import get_default_qat_qconfig\n",
    "try:\n",
    "    from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
    "except ImportError:\n",
    "    from torch.ao.quantization.fx import prepare_qat_fx, convert_fx  # type: ignore\n",
    "from torch.ao.quantization.qconfig_mapping import QConfigMapping\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_BASELINE = 100\n",
    "LR_BASELINE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "# QAT: fine-tuning curto\n",
    "EPOCHS_QAT = 10         # ajuste conforme necessidade (5‚Äì20 costuma bastar)\n",
    "LR_QAT = 3e-4           # LR menor para estabilidade durante QAT\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # CPU\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelo -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 16*length)\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)             # manter em float na QAT/PTQ\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)    # (N,)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_model(model, loader, epochs, lr):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:02d}/{epochs}  - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def fine_tune_qat(model_qat, loader, epochs, lr):\n",
    "    \"\"\"Fine-tuning com fake-quant (QAT).\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_qat.parameters(), lr=lr)\n",
    "    model_qat.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model_qat(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        print(f\"[QAT] Epoch {epoch:02d}/{epochs} - Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ============================================================\n",
    "    # PR√â-PROCESSAMENTO (igual ao baseline)\n",
    "    # ============================================================\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) Treino BASELINE (FP32)\n",
    "    # ============================================================\n",
    "    model = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando baseline (FP32) ===\")\n",
    "    train_model(model, train_loader, epochs=EPOCHS_BASELINE, lr=LR_BASELINE)\n",
    "    evaluate_model(model, test_loader, label=\"Baseline FP32\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 2) QAT: preparar modelo com fake-quant, fine-tuning e convers√£o\n",
    "    # ============================================================\n",
    "    print(\"\\n=== QAT (fake-quant) + Convers√£o INT8 ===\")\n",
    "\n",
    "    # Backend de quantiza√ß√£o adequado\n",
    "    engines = torch.backends.quantized.supported_engines\n",
    "    engine = \"fbgemm\" if \"fbgemm\" in engines else (\"qnnpack\" if \"qnnpack\" in engines else None)\n",
    "    if engine is None:\n",
    "        raise RuntimeError(f\"Nenhum backend de quantiza√ß√£o suportado encontrado: {engines}\")\n",
    "    torch.backends.quantized.engine = engine\n",
    "    print(f\"Backend de quantiza√ß√£o: {engine}\")\n",
    "\n",
    "    # QConfig para QAT e mapeamento (exclui √∫ltima camada)\n",
    "    from torch.ao.quantization import get_default_qat_qconfig, disable_observer\n",
    "    from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "    qconfig = get_default_qat_qconfig(engine)\n",
    "    qconfig_mapping = (\n",
    "        QConfigMapping()\n",
    "        .set_global(qconfig)              # aplica a todos\n",
    "        .set_module_name(\"net.7\", None)   # N√ÉO quantizar a √∫ltima Linear\n",
    "    )\n",
    "\n",
    "    example_input = torch.randn(1, 1, X_train_cnn.shape[2])\n",
    "\n",
    "    # **PREPARE EM MODO TREINO**\n",
    "    model.train()\n",
    "    model_qat = prepare_qat_fx(model, qconfig_mapping, example_inputs=example_input)\n",
    "\n",
    "    # Fine-tuning QAT com congelamento de observadores na metade\n",
    "    EPOCHS_QAT = 15\n",
    "    LR_QAT = 1e-4\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_qat.parameters(), lr=LR_QAT)\n",
    "\n",
    "    for epoch in range(1, EPOCHS_QAT + 1):\n",
    "        model_qat.train()\n",
    "        if epoch == (EPOCHS_QAT // 2) + 1:\n",
    "            # congela ranges (scales/zero-points) para estabilizar o resto do treino\n",
    "            disable_observer(model_qat)\n",
    "\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            pred = model_qat(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # clipping leve para estabilidade\n",
    "            clip_grad_norm_(model_qat.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running += loss.item() * xb.size(0)\n",
    "        running /= len(train_loader.dataset)\n",
    "        print(f\"[QAT] Epoch {epoch:02d}/{EPOCHS_QAT} - Train MSE: {running:.6f}\")\n",
    "\n",
    "    # Converter para quantizado real (INT8 onde suportado)\n",
    "    model_qat.eval()\n",
    "    quantized_model = convert_fx(model_qat).to(DEVICE).eval()\n",
    "\n",
    "    # Avaliar modelo quantizado\n",
    "    evaluate_model(quantized_model, test_loader, label=\"QAT ‚Üí INT8 (fix)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c40b8",
   "metadata": {},
   "source": [
    "# Otimiza√ß√£o 3: Knowledge Distillation\n",
    "1. Response KD com Huber\n",
    "2. Feature KD (uma camada) com projetor 1x1\n",
    "3. Relational KD no embedding (p√≥s-flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c57c8c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando TEACHER (baseline) ===\n",
      "[Teacher] Epoch 01/100 - MSE: 6.248398\n",
      "[Teacher] Epoch 05/100 - MSE: 0.270467\n",
      "[Teacher] Epoch 10/100 - MSE: 0.262171\n",
      "[Teacher] Epoch 15/100 - MSE: 0.258560\n",
      "[Teacher] Epoch 20/100 - MSE: 0.264250\n",
      "[Teacher] Epoch 25/100 - MSE: 0.264546\n",
      "[Teacher] Epoch 30/100 - MSE: 0.257848\n",
      "[Teacher] Epoch 35/100 - MSE: 0.268403\n",
      "[Teacher] Epoch 40/100 - MSE: 0.255421\n",
      "[Teacher] Epoch 45/100 - MSE: 0.264791\n",
      "[Teacher] Epoch 50/100 - MSE: 0.259054\n",
      "[Teacher] Epoch 55/100 - MSE: 0.253974\n",
      "[Teacher] Epoch 60/100 - MSE: 0.256649\n",
      "[Teacher] Epoch 65/100 - MSE: 0.261080\n",
      "[Teacher] Epoch 70/100 - MSE: 0.255296\n",
      "[Teacher] Epoch 75/100 - MSE: 0.252065\n",
      "[Teacher] Epoch 80/100 - MSE: 0.255939\n",
      "[Teacher] Epoch 85/100 - MSE: 0.253428\n",
      "[Teacher] Epoch 90/100 - MSE: 0.254143\n",
      "[Teacher] Epoch 95/100 - MSE: 0.256427\n",
      "[Teacher] Epoch 100/100 - MSE: 0.253061\n",
      "\n",
      "üìä Teacher (FP32):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.575093 s\n",
      "Tempo m√©dio por amostra:   0.0957 ms\n",
      "\n",
      "=== Treinando STUDENT com Response KD (Huber) ===\n",
      "[KD-Response] Epoch 01/60 - Loss: 0.633658\n",
      "[KD-Response] Epoch 05/60 - Loss: 0.017945\n",
      "[KD-Response] Epoch 10/60 - Loss: 0.016557\n",
      "[KD-Response] Epoch 15/60 - Loss: 0.016040\n",
      "[KD-Response] Epoch 20/60 - Loss: 0.016285\n",
      "[KD-Response] Epoch 25/60 - Loss: 0.015882\n",
      "[KD-Response] Epoch 30/60 - Loss: 0.015881\n",
      "[KD-Response] Epoch 35/60 - Loss: 0.015643\n",
      "[KD-Response] Epoch 40/60 - Loss: 0.015702\n",
      "[KD-Response] Epoch 45/60 - Loss: 0.015621\n",
      "[KD-Response] Epoch 50/60 - Loss: 0.015545\n",
      "[KD-Response] Epoch 55/60 - Loss: 0.015435\n",
      "[KD-Response] Epoch 60/60 - Loss: 0.015375\n",
      "\n",
      "üìä Student (KD-Response, Œ±=0.5, Œ¥=1.0):\n",
      "MAE:   0.0995\n",
      "RMSE:  1.0598\n",
      "R¬≤:    0.9336\n",
      "Tempo total de infer√™ncia: 0.172024 s\n",
      "Tempo m√©dio por amostra:   0.0286 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_kd_response.py\n",
    "# ============================================================\n",
    "# Regress√£o \"Precipita√ß√£o dia\" - Knowledge Distillation (Response KD + Huber)\n",
    "# - Teacher = sua CNN baseline (FP32)\n",
    "# - Student = CNN menor\n",
    "# - Loss: L = Œ±*Huber(y, y_s) + (1-Œ±)*Huber(y_t, y_s)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_TEACHER = 100\n",
    "LR_TEACHER = 1e-3\n",
    "\n",
    "EPOCHS_STUDENT = 60\n",
    "LR_STUDENT = 1e-3\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "ALPHA = 0.5        # peso da perda supervisionada vs distila√ß√£o\n",
    "HUBER_DELTA = 1.0  # delta da HuberLoss\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # CPU\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelos -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    \"\"\"Teacher (baseline)\"\"\"\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 16*length)\n",
    "            nn.Linear(16 * length, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "class CNNRegressorStudent(nn.Module):\n",
    "    \"\"\"Student menor\"\"\"\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                 # (N, 8*length)\n",
    "            nn.Linear(8 * length, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_supervised(model, loader, epochs, lr, label=\"Train\"):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"[{label}] Epoch {epoch:02d}/{epochs} - MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def train_student_kd_response(student, teacher, loader, epochs, lr, alpha=0.5, delta=1.0):\n",
    "    \"\"\"\n",
    "    Response KD para regress√£o com Huber:\n",
    "    L = Œ±*Huber(y, y_s) + (1-Œ±)*Huber(y_t, y_s)\n",
    "    \"\"\"\n",
    "    huber = nn.HuberLoss(delta=delta)\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "\n",
    "    teacher.eval()\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        student.train()\n",
    "        run_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            with torch.no_grad():\n",
    "                y_teacher = teacher(xb)  # alvo suave\n",
    "\n",
    "            y_student = student(xb)\n",
    "            loss_sup  = huber(y_student, yb)\n",
    "            loss_dist = huber(y_student, y_teacher)\n",
    "            loss = alpha * loss_sup + (1.0 - alpha) * loss_dist\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            run_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        run_loss /= len(loader.dataset)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"[KD-Response] Epoch {epoch:02d}/{epochs} - Loss: {run_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ============================================================\n",
    "    # PR√â-PROCESSAMENTO (mesmo do baseline)\n",
    "    # ============================================================\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) Teacher: treino e avalia√ß√£o\n",
    "    # ============================================================\n",
    "    teacher = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando TEACHER (baseline) ===\")\n",
    "    train_supervised(teacher, train_loader, epochs=EPOCHS_TEACHER, lr=LR_TEACHER, label=\"Teacher\")\n",
    "    evaluate_model(teacher, test_loader, label=\"Teacher (FP32)\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 2) Student: KD (Response + Huber)\n",
    "    # ============================================================\n",
    "    student = CNNRegressorStudent(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando STUDENT com Response KD (Huber) ===\")\n",
    "    train_student_kd_response(student, teacher, train_loader,\n",
    "                              epochs=EPOCHS_STUDENT, lr=LR_STUDENT,\n",
    "                              alpha=ALPHA, delta=HUBER_DELTA)\n",
    "    evaluate_model(student, test_loader, label=f\"Student (KD-Response, Œ±={ALPHA}, Œ¥={HUBER_DELTA})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a8a0f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando TEACHER (baseline) ===\n",
      "[Teacher] Epoch 01/100 - MSE: 6.248398\n",
      "[Teacher] Epoch 05/100 - MSE: 0.270467\n",
      "[Teacher] Epoch 10/100 - MSE: 0.262171\n",
      "[Teacher] Epoch 15/100 - MSE: 0.258560\n",
      "[Teacher] Epoch 20/100 - MSE: 0.264250\n",
      "[Teacher] Epoch 25/100 - MSE: 0.264546\n",
      "[Teacher] Epoch 30/100 - MSE: 0.257848\n",
      "[Teacher] Epoch 35/100 - MSE: 0.268403\n",
      "[Teacher] Epoch 40/100 - MSE: 0.255421\n",
      "[Teacher] Epoch 45/100 - MSE: 0.264791\n",
      "[Teacher] Epoch 50/100 - MSE: 0.259054\n",
      "[Teacher] Epoch 55/100 - MSE: 0.253974\n",
      "[Teacher] Epoch 60/100 - MSE: 0.256649\n",
      "[Teacher] Epoch 65/100 - MSE: 0.261080\n",
      "[Teacher] Epoch 70/100 - MSE: 0.255296\n",
      "[Teacher] Epoch 75/100 - MSE: 0.252065\n",
      "[Teacher] Epoch 80/100 - MSE: 0.255939\n",
      "[Teacher] Epoch 85/100 - MSE: 0.253428\n",
      "[Teacher] Epoch 90/100 - MSE: 0.254143\n",
      "[Teacher] Epoch 95/100 - MSE: 0.256427\n",
      "[Teacher] Epoch 100/100 - MSE: 0.253061\n",
      "\n",
      "üìä Teacher (FP32):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.448194 s\n",
      "Tempo m√©dio por amostra:   0.0746 ms\n",
      "\n",
      "=== Treinando STUDENT com Feature KD (1√ó1) + Response KD ===\n",
      "[KD-Feature] Epoch 01/60 - Loss: 0.652360\n",
      "[KD-Feature] Epoch 05/60 - Loss: 0.046768\n",
      "[KD-Feature] Epoch 10/60 - Loss: 0.040565\n",
      "[KD-Feature] Epoch 15/60 - Loss: 0.038369\n",
      "[KD-Feature] Epoch 20/60 - Loss: 0.037220\n",
      "[KD-Feature] Epoch 25/60 - Loss: 0.036231\n",
      "[KD-Feature] Epoch 30/60 - Loss: 0.035686\n",
      "[KD-Feature] Epoch 35/60 - Loss: 0.035085\n",
      "[KD-Feature] Epoch 40/60 - Loss: 0.034777\n",
      "[KD-Feature] Epoch 45/60 - Loss: 0.034573\n",
      "[KD-Feature] Epoch 50/60 - Loss: 0.034404\n",
      "[KD-Feature] Epoch 55/60 - Loss: 0.034288\n",
      "[KD-Feature] Epoch 60/60 - Loss: 0.034361\n",
      "\n",
      "üìä Student (Feature KD + Response KD, Œ±=0.5, Œ≤=0.1, Œ¥=1.0):\n",
      "MAE:   0.1088\n",
      "RMSE:  1.0669\n",
      "R¬≤:    0.9327\n",
      "Tempo total de infer√™ncia: 0.421808 s\n",
      "Tempo m√©dio por amostra:   0.0702 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_kd_feature.py\n",
    "# ============================================================\n",
    "# Regress√£o \"Precipita√ß√£o dia\" - Knowledge Distillation (Feature KD + Response KD)\n",
    "# - Teacher = sua CNN baseline (FP32)\n",
    "# - Student = CNN menor\n",
    "# - Loss total:\n",
    "#   L = Œ±*Huber(y, y_s) + (1-Œ±)*Huber(y_t, y_s) + Œ≤*MSE( f_T, P(f_S) )\n",
    "#     onde P √© um projetor 1x1 que mapeia canais do student para os do teacher\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_TEACHER = 100\n",
    "LR_TEACHER = 1e-3\n",
    "\n",
    "EPOCHS_STUDENT = 60\n",
    "LR_STUDENT = 1e-3\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "# Pesos das perdas\n",
    "ALPHA = 0.5       # supervisionada vs resposta do teacher\n",
    "BETA = 0.1        # termo de feature KD\n",
    "HUBER_DELTA = 1.0\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # CPU\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelos -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    \"\"\"Teacher (baseline)\"\"\"\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),  # 0\n",
    "            nn.ReLU(),                                                           # 1\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1), # 2\n",
    "            nn.ReLU(),                                                           # 3 <- capturar aqui (p√≥s 2¬™ Conv)\n",
    "            nn.Flatten(),                                                        # 4\n",
    "            nn.Linear(16 * length, 32),                                          # 5\n",
    "            nn.ReLU(),                                                           # 6\n",
    "            nn.Linear(32, 1)                                                     # 7\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "class CNNRegressorStudent(nn.Module):\n",
    "    \"\"\"Student menor (menos canais e hidden reduzido)\"\"\"\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, padding=1),  # 0\n",
    "            nn.ReLU(),                                                           # 1\n",
    "            nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, padding=1),  # 2\n",
    "            nn.ReLU(),                                                           # 3 <- capturar aqui (p√≥s 2¬™ Conv)\n",
    "            nn.Flatten(),                                                        # 4\n",
    "            nn.Linear(8 * length, 16),                                           # 5\n",
    "            nn.ReLU(),                                                           # 6\n",
    "            nn.Linear(16, 1)                                                     # 7\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# ----------------------- Treino / Avalia√ß√£o -----------------------\n",
    "def train_supervised(model, loader, epochs, lr, label=\"Train\"):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"[{label}] Epoch {epoch:02d}/{epochs} - MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- KD (Response + Feature 1√ó1) -----------------------\n",
    "def train_student_kd_feature(student, teacher, loader, epochs, lr, alpha=0.5, beta=0.1, delta=1.0):\n",
    "    \"\"\"\n",
    "    Treina student com:\n",
    "      - Response KD (Huber) entre y_teacher e y_student\n",
    "      - Feature KD (MSE) entre f_T e P(f_S), capturadas p√≥s 2¬™ Conv (√≠ndice 3 nas sequentials)\n",
    "    \"\"\"\n",
    "    huber = nn.HuberLoss(delta=delta)\n",
    "    feat_mse = nn.MSELoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "\n",
    "    # Congela teacher\n",
    "    teacher.eval()\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Projetor 1x1 para casar canais: student C=8 -> teacher C=16\n",
    "    projector = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=1, bias=False).to(DEVICE)\n",
    "\n",
    "    # Hooks para capturar features (p√≥s 2¬™ Conv: √≠ndice 3 = ReLU)\n",
    "    t_feat = {\"val\": None}\n",
    "    s_feat = {\"val\": None}\n",
    "\n",
    "    def t_hook(_m, _inp, out): t_feat.update(val=out.detach())\n",
    "    def s_hook(_m, _inp, out): s_feat.update(val=out)\n",
    "\n",
    "    t_handle = teacher.net[3].register_forward_hook(t_hook)\n",
    "    s_handle = student.net[3].register_forward_hook(s_hook)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            student.train()\n",
    "            projector.train()\n",
    "            run_loss = 0.0\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                # Forward teacher (sem grad)\n",
    "                with torch.no_grad():\n",
    "                    y_teacher = teacher(xb)\n",
    "\n",
    "                # Forward student (hooks capturam s_feat['val'])\n",
    "                y_student = student(xb)\n",
    "\n",
    "                # Response KD (sa√≠das)\n",
    "                loss_sup  = huber(y_student, yb)\n",
    "                loss_dist = huber(y_student, y_teacher)\n",
    "\n",
    "                # Feature KD: projeta feature do student para os canais do teacher e compara\n",
    "                if (t_feat[\"val\"] is None) or (s_feat[\"val\"] is None):\n",
    "                    # Seguran√ßa: se por algum motivo o hook n√£o populou, faz um forward ‚Äúseco‚Äù\n",
    "                    with torch.no_grad():\n",
    "                        teacher(xb)\n",
    "                    _ = student(xb)\n",
    "\n",
    "                proj_s = projector(s_feat[\"val\"])\n",
    "                loss_feat = feat_mse(proj_s, t_feat[\"val\"])\n",
    "\n",
    "                loss = alpha * loss_sup + (1.0 - alpha) * loss_dist + beta * loss_feat\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                run_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            run_loss /= len(loader.dataset)\n",
    "            if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "                print(f\"[KD-Feature] Epoch {epoch:02d}/{epochs} - Loss: {run_loss:.6f}\")\n",
    "\n",
    "    finally:\n",
    "        # Limpa hooks\n",
    "        t_handle.remove()\n",
    "        s_handle.remove()\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ============================================================\n",
    "    # PR√â-PROCESSAMENTO (igual ao baseline)\n",
    "    # ============================================================\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) Teacher: treino e avalia√ß√£o (FP32)\n",
    "    # ============================================================\n",
    "    teacher = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando TEACHER (baseline) ===\")\n",
    "    train_supervised(teacher, train_loader, epochs=EPOCHS_TEACHER, lr=LR_TEACHER, label=\"Teacher\")\n",
    "    evaluate_model(teacher, test_loader, label=\"Teacher (FP32)\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 2) Student: KD (Response + Feature 1√ó1)\n",
    "    # ============================================================\n",
    "    student = CNNRegressorStudent(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando STUDENT com Feature KD (1√ó1) + Response KD ===\")\n",
    "    train_student_kd_feature(student, teacher, train_loader,\n",
    "                             epochs=EPOCHS_STUDENT, lr=LR_STUDENT,\n",
    "                             alpha=ALPHA, beta=BETA, delta=HUBER_DELTA)\n",
    "\n",
    "    evaluate_model(student, test_loader,\n",
    "                   label=f\"Student (Feature KD + Response KD, Œ±={ALPHA}, Œ≤={BETA}, Œ¥={HUBER_DELTA})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cba319b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treinando TEACHER (baseline) ===\n",
      "[Teacher] Epoch 01/100 - MSE: 6.248398\n",
      "[Teacher] Epoch 05/100 - MSE: 0.270467\n",
      "[Teacher] Epoch 10/100 - MSE: 0.262171\n",
      "[Teacher] Epoch 15/100 - MSE: 0.258560\n",
      "[Teacher] Epoch 20/100 - MSE: 0.264250\n",
      "[Teacher] Epoch 25/100 - MSE: 0.264546\n",
      "[Teacher] Epoch 30/100 - MSE: 0.257848\n",
      "[Teacher] Epoch 35/100 - MSE: 0.268403\n",
      "[Teacher] Epoch 40/100 - MSE: 0.255421\n",
      "[Teacher] Epoch 45/100 - MSE: 0.264791\n",
      "[Teacher] Epoch 50/100 - MSE: 0.259054\n",
      "[Teacher] Epoch 55/100 - MSE: 0.253974\n",
      "[Teacher] Epoch 60/100 - MSE: 0.256649\n",
      "[Teacher] Epoch 65/100 - MSE: 0.261080\n",
      "[Teacher] Epoch 70/100 - MSE: 0.255296\n",
      "[Teacher] Epoch 75/100 - MSE: 0.252065\n",
      "[Teacher] Epoch 80/100 - MSE: 0.255939\n",
      "[Teacher] Epoch 85/100 - MSE: 0.253428\n",
      "[Teacher] Epoch 90/100 - MSE: 0.254143\n",
      "[Teacher] Epoch 95/100 - MSE: 0.256427\n",
      "[Teacher] Epoch 100/100 - MSE: 0.253061\n",
      "\n",
      "üìä Teacher (FP32):\n",
      "MAE:   0.1425\n",
      "RMSE:  1.0632\n",
      "R¬≤:    0.9332\n",
      "Tempo total de infer√™ncia: 0.185584 s\n",
      "Tempo m√©dio por amostra:   0.0309 ms\n",
      "\n",
      "=== Treinando STUDENT com RKD (Response + Distance + Angle) ===\n",
      "[KD-RKD] Epoch 01/60 - Loss: 0.654376\n",
      "[KD-RKD] Epoch 05/60 - Loss: 0.023009\n",
      "[KD-RKD] Epoch 10/60 - Loss: 0.019148\n",
      "[KD-RKD] Epoch 15/60 - Loss: 0.017908\n",
      "[KD-RKD] Epoch 20/60 - Loss: 0.017778\n",
      "[KD-RKD] Epoch 25/60 - Loss: 0.017199\n",
      "[KD-RKD] Epoch 30/60 - Loss: 0.017656\n",
      "[KD-RKD] Epoch 35/60 - Loss: 0.016924\n",
      "[KD-RKD] Epoch 40/60 - Loss: 0.017076\n",
      "[KD-RKD] Epoch 45/60 - Loss: 0.016833\n",
      "[KD-RKD] Epoch 50/60 - Loss: 0.016616\n",
      "[KD-RKD] Epoch 55/60 - Loss: 0.016647\n",
      "[KD-RKD] Epoch 60/60 - Loss: 0.016474\n",
      "\n",
      "üìä Student (RKD: Œ±=0.5, Œ≤=0.1, Œ≥=1.0, Œ¥=1.0):\n",
      "MAE:   0.0997\n",
      "RMSE:  1.0599\n",
      "R¬≤:    0.9336\n",
      "Tempo total de infer√™ncia: 0.160814 s\n",
      "Tempo m√©dio por amostra:   0.0268 ms\n"
     ]
    }
   ],
   "source": [
    "# cnn_regressao_kd_rkd.py\n",
    "# ============================================================\n",
    "# Regress√£o \"Precipita√ß√£o dia\" - Knowledge Distillation (RKD)\n",
    "# - Teacher = sua CNN baseline (FP32)\n",
    "# - Student = CNN menor\n",
    "# - Loss total:\n",
    "#   L = Œ±*Huber(y, y_s) + (1-Œ±)*Huber(y_t, y_s) + Œ≤*L_RKD\n",
    "#   onde L_RKD = L_dist(pairwise) + Œ≥*L_angle(sampled triplets)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------- Configura√ß√£o -----------------------\n",
    "CSV_PATH = \"bd_EstacaoVargemFria_e_Pesca.csv\"\n",
    "\n",
    "EPOCHS_TEACHER = 100\n",
    "LR_TEACHER = 1e-3\n",
    "\n",
    "EPOCHS_STUDENT = 60\n",
    "LR_STUDENT = 1e-3\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "# Pesos das perdas\n",
    "ALPHA = 0.5        # supervisionada vs resposta do teacher\n",
    "BETA = 0.1         # peso do termo RKD\n",
    "GAMMA = 1.0        # peso relativo do termo de √¢ngulo dentro do RKD\n",
    "HUBER_DELTA = 1.0\n",
    "\n",
    "# Triplets por minibatch para o termo de √¢ngulo (amostragem)\n",
    "MAX_TRIPLETS = 2048\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # CPU\n",
    "\n",
    "# ----------------------- Utilidades -----------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_cnn1d_shape(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    return x.reshape(x.shape[0], 1, x.shape[1])  # (N, 1, F)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_pred - y_true)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "\n",
    "def r2_score_np(y_true, y_pred):\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n",
    "\n",
    "# ----------------------- Dataset -----------------------\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)                         # (N, 1, F)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32).reshape(-1))  # (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------- Modelos -----------------------\n",
    "class CNNRegressor(nn.Module):\n",
    "    \"\"\"Teacher (baseline)\"\"\"\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),  # 0\n",
    "            nn.ReLU(),                                                           # 1\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1), # 2\n",
    "            nn.ReLU(),                                                           # 3\n",
    "            nn.Flatten(),                                                        # 4  <- EMBEDDING AQUI\n",
    "            nn.Linear(16 * length, 32),                                          # 5\n",
    "            nn.ReLU(),                                                           # 6\n",
    "            nn.Linear(32, 1)                                                     # 7\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "class CNNRegressorStudent(nn.Module):\n",
    "    \"\"\"Student menor (menos canais e hidden reduzido)\"\"\"\n",
    "    def __init__(self, in_channels=1, length=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, padding=1),  # 0\n",
    "            nn.ReLU(),                                                           # 1\n",
    "            nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, padding=1),  # 2\n",
    "            nn.ReLU(),                                                           # 3\n",
    "            nn.Flatten(),                                                        # 4  <- EMBEDDING AQUI\n",
    "            nn.Linear(8 * length, 16),                                           # 5\n",
    "            nn.ReLU(),                                                           # 6\n",
    "            nn.Linear(16, 1)                                                     # 7\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# ----------------------- M√©tricas -----------------------\n",
    "def evaluate_model(model, loader, label=\"Avalia√ß√£o\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb).cpu().numpy()\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(yb.cpu().numpy())\n",
    "    t1 = time.time()\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    _rmse = rmse(y_true, y_pred)\n",
    "    _r2 = r2_score_np(y_true, y_pred)\n",
    "    total_time = t1 - t0\n",
    "    time_per_sample_ms = (total_time / len(loader.dataset)) * 1000.0\n",
    "\n",
    "    print(f\"\\nüìä {label}:\")\n",
    "    print(f\"MAE:   {_mae:.4f}\")\n",
    "    print(f\"RMSE:  {_rmse:.4f}\")\n",
    "    print(f\"R¬≤:    {_r2:.4f}\")\n",
    "    print(f\"Tempo total de infer√™ncia: {total_time:.6f} s\")\n",
    "    print(f\"Tempo m√©dio por amostra:   {time_per_sample_ms:.4f} ms\")\n",
    "\n",
    "    return _mae, _rmse, _r2, time_per_sample_ms\n",
    "\n",
    "# ----------------------- RKD helpers -----------------------\n",
    "def pairwise_distance_matrix(x: torch.Tensor, eps: float = 1e-12):\n",
    "    # x: (B, D) -> D_ij = ||x_i - x_j||\n",
    "    d = torch.cdist(x, x)  # (B, B)\n",
    "    mean = d[d > 0].mean() if (d > 0).any() else torch.tensor(1.0, device=x.device)\n",
    "    return d / (mean + eps)\n",
    "\n",
    "def rkd_distance_loss(e_s: torch.Tensor, e_t: torch.Tensor):\n",
    "    ds = pairwise_distance_matrix(e_s)\n",
    "    dt = pairwise_distance_matrix(e_t)\n",
    "    # compara apenas parte superior (i<j) para n√£o duplicar\n",
    "    idx = torch.triu_indices(ds.size(0), ds.size(1), offset=1)\n",
    "    return nn.functional.mse_loss(ds[idx[0], idx[1]], dt[idx[0], idx[1]])\n",
    "\n",
    "def sample_triplets(B: int, max_triplets: int, device):\n",
    "    # amostra tr√≠ades (i, j, k) distintas\n",
    "    import math, random\n",
    "    n_all = B * (B - 1) * (B - 2) // 6  # aprox (i<j<k)\n",
    "    t = min(max_triplets, n_all) if n_all > 0 else 0\n",
    "    if t == 0:\n",
    "        return None\n",
    "    triplets = set()\n",
    "    while len(triplets) < t:\n",
    "        i, j, k = np.random.choice(B, 3, replace=False)\n",
    "        triplets.add(tuple(sorted((int(i), int(j), int(k)))))\n",
    "    triplets = np.array(list(triplets), dtype=np.int64)\n",
    "    return torch.from_numpy(triplets).to(device)\n",
    "\n",
    "def rkd_angle_loss(e_s: torch.Tensor, e_t: torch.Tensor, max_triplets=2048):\n",
    "    \"\"\"\n",
    "    Define √¢ngulos usando tr√≠ades (i, j, k): ‚à†(x_j - x_i, x_k - x_i)\n",
    "    Compara cosenos dos √¢ngulos (teacher vs student).\n",
    "    \"\"\"\n",
    "    B = e_s.size(0)\n",
    "    tri = sample_triplets(B, max_triplets, e_s.device)\n",
    "    if tri is None:\n",
    "        return torch.tensor(0.0, device=e_s.device)\n",
    "\n",
    "    i = tri[:, 0]; j = tri[:, 1]; k = tri[:, 2]\n",
    "\n",
    "    def angle_cos(e):\n",
    "        v1 = e[j] - e[i]  # (t, D)\n",
    "        v2 = e[k] - e[i]\n",
    "        v1 = nn.functional.normalize(v1, dim=1)\n",
    "        v2 = nn.functional.normalize(v2, dim=1)\n",
    "        return (v1 * v2).sum(dim=1)  # cosenos (t,)\n",
    "    cs = angle_cos(e_s)\n",
    "    ct = angle_cos(e_t)\n",
    "    return nn.functional.mse_loss(cs, ct)\n",
    "\n",
    "# ----------------------- Treinos -----------------------\n",
    "def train_supervised(model, loader, epochs, lr, label=\"Train\"):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"[{label}] Epoch {epoch:02d}/{epochs} - MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "def train_student_kd_rkd(student, teacher, loader, epochs, lr,\n",
    "                         alpha=0.5, beta=0.1, gamma=1.0, delta=1.0,\n",
    "                         max_triplets=2048):\n",
    "    \"\"\"\n",
    "    L = Œ±*Huber(y, y_s) + (1-Œ±)*Huber(y_t, y_s) + Œ≤*( L_dist + Œ≥*L_angle )\n",
    "    - Embeddings: sa√≠da do Flatten (√≠ndice 4 nos sequentials)\n",
    "    \"\"\"\n",
    "    huber = nn.HuberLoss(delta=delta)\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "\n",
    "    # Hooks para capturar embeddings (p√≥s Flatten)\n",
    "    t_emb = {\"val\": None}\n",
    "    s_emb = {\"val\": None}\n",
    "    def t_hook(_m, _i, o): t_emb.update(val=o.detach())\n",
    "    def s_hook(_m, _i, o): s_emb.update(val=o)\n",
    "\n",
    "    t_handle = teacher.net[4].register_forward_hook(t_hook)\n",
    "    s_handle = student.net[4].register_forward_hook(s_hook)\n",
    "\n",
    "    # Teacher congelado\n",
    "    teacher.eval()\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    try:\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            student.train()\n",
    "            run_loss = 0.0\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                with torch.no_grad():\n",
    "                    y_teacher = teacher(xb)\n",
    "\n",
    "                y_student = student(xb)\n",
    "\n",
    "                # KD de resposta (Huber)\n",
    "                loss_sup  = huber(y_student, yb)\n",
    "                loss_distill = huber(y_student, y_teacher)\n",
    "\n",
    "                # RKD: usa embeddings capturados pelos hooks\n",
    "                if (t_emb[\"val\"] is None) or (s_emb[\"val\"] is None):\n",
    "                    with torch.no_grad():\n",
    "                        teacher(xb)\n",
    "                    _ = student(xb)\n",
    "\n",
    "                e_t = t_emb[\"val\"]  # (B, D_t)\n",
    "                e_s = s_emb[\"val\"]  # (B, D_s)\n",
    "\n",
    "                # Dist√¢ncias (pairwise)\n",
    "                Ld = rkd_distance_loss(e_s, e_t)\n",
    "\n",
    "                # √Çngulos (tr√≠ades amostradas)\n",
    "                La = rkd_angle_loss(e_s, e_t, max_triplets=max_triplets)\n",
    "\n",
    "                L_rkd = Ld + gamma * La\n",
    "\n",
    "                loss = alpha * loss_sup + (1.0 - alpha) * loss_distill + beta * L_rkd\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                run_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            run_loss /= len(loader.dataset)\n",
    "            if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "                print(f\"[KD-RKD] Epoch {epoch:02d}/{epochs} - Loss: {run_loss:.6f}\")\n",
    "\n",
    "    finally:\n",
    "        t_handle.remove()\n",
    "        s_handle.remove()\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ============================\n",
    "    # PR√â-PROCESSAMENTO (original)\n",
    "    # ============================\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = df[df['Nome'].str.strip() == 'Esta√ß√£o Pesca - UFRPE']\n",
    "    df['Data esta√ß√£o'] = pd.to_datetime(df['Data esta√ß√£o'], errors='coerce')\n",
    "    df = df.sort_values('Data esta√ß√£o')\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')\n",
    "    df['Precipita√ß√£o anterior'] = df['Precipita√ß√£o dia'].shift(1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    colunas_features = ['Temperatura', 'Umidade', 'Velocidade Vento', 'Rajada Vento', 'Precipita√ß√£o anterior']\n",
    "    coluna_saida = 'Precipita√ß√£o dia'\n",
    "    df = df[colunas_features + [coluna_saida]]\n",
    "\n",
    "    X = df[colunas_features].values\n",
    "    y = df[coluna_saida].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X_train_cnn = to_cnn1d_shape(X_train)   # (N, 1, 5)\n",
    "    X_test_cnn  = to_cnn1d_shape(X_test)    # (N, 1, 5)\n",
    "\n",
    "    train_ds = WeatherDataset(X_train_cnn, y_train)\n",
    "    test_ds  = WeatherDataset(X_test_cnn,  y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ============================\n",
    "    # 1) Teacher: treino + avalia√ß√£o\n",
    "    # ============================\n",
    "    teacher = CNNRegressor(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando TEACHER (baseline) ===\")\n",
    "    train_supervised(teacher, train_loader, epochs=EPOCHS_TEACHER, lr=LR_TEACHER, label=\"Teacher\")\n",
    "    evaluate_model(teacher, test_loader, label=\"Teacher (FP32)\")\n",
    "\n",
    "    # ============================\n",
    "    # 2) Student: RKD (Response + Dist√¢ncia + √Çngulo)\n",
    "    # ============================\n",
    "    student = CNNRegressorStudent(in_channels=1, length=X_train_cnn.shape[2]).to(DEVICE)\n",
    "    print(\"\\n=== Treinando STUDENT com RKD (Response + Distance + Angle) ===\")\n",
    "    train_student_kd_rkd(\n",
    "        student, teacher, train_loader,\n",
    "        epochs=EPOCHS_STUDENT, lr=LR_STUDENT,\n",
    "        alpha=ALPHA, beta=BETA, gamma=GAMMA, delta=HUBER_DELTA,\n",
    "        max_triplets=MAX_TRIPLETS\n",
    "    )\n",
    "    evaluate_model(student, test_loader,\n",
    "                   label=f\"Student (RKD: Œ±={ALPHA}, Œ≤={BETA}, Œ≥={GAMMA}, Œ¥={HUBER_DELTA})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
